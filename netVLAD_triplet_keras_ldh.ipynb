{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "netVLAD_triplet_keras_ldh.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glee1228/net_VLad_keras/blob/master/netVLAD_triplet_keras_ldh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcX-54sSCQf9",
        "colab_type": "text"
      },
      "source": [
        "## 코랩 연동"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOMyGNH7ULRF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3252cd4-190b-4e84-8eaf-294caa8d68d6"
      },
      "source": [
        "from keras.applications.xception import Xception\n",
        "from keras.applications.densenet import DenseNet121, DenseNet169, DenseNet201\n",
        "from keras.applications.nasnet import NASNetMobile\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.applications import VGG16\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD8vfj9mX33K",
        "colab_type": "code",
        "outputId": "f0f57d63-ff2d-4ea4-d94a-0d3e60efe32a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install tensorboardcolab\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "tb = TensorBoardColab()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n",
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://b0eb4050.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu1jT3RYwxeQ",
        "colab_type": "code",
        "outputId": "71406fff-1816-424a-c34e-34cf359f9bf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if os.path.exists('/content/gdrive')==False:\n",
        "    drive.mount('/content/gdrive')\n",
        "    print('Google Drive is mounted\\n')\n",
        "else:\n",
        "    print('Google Drive is already mounted\\n')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google Drive is already mounted\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhko5ZnxYM3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.set_random_seed(777)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVbt03PPCUgG",
        "colab_type": "text"
      },
      "source": [
        "## Data Path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7qv_b9jUqAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prof_team = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVXFtGxtVGPx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a325ae18-8b94-46e0-c989-a651854ab3be"
      },
      "source": [
        "if prof_team :\n",
        "  ckpt_path = '/content/gdrive/My Drive/AILeader_Dataset/Checkpoint_jw'\n",
        "  train_path = './gdrive/My Drive/AILeader_Dataset/train'\n",
        "  test_path = './gdrive/My Drive/AILeader_Dataset/test'\n",
        "  ! ls -a ./gdrive/My\\ Drive/AILeader_Dataset/train\n",
        "  \n",
        "else :\n",
        "  ckpt_path = './gdrive/My Drive/ckpt'\n",
        "  train_path = './gdrive/My Drive/train'\n",
        "  test_path = './gdrive/My Drive/test'\n",
        "  ! ls -a ./gdrive/My\\ Drive/train"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " AI\t        dorm\t\t   'Front door Sejong'\t'Stone statue'\n",
            "'Clock tower'  'Front door Child'   Museum\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohZAm7Ugw01q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES']= '0'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn1lec3N5bcC",
        "colab_type": "text"
      },
      "source": [
        "## NetVLAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tcd0GVzj5d7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/mpkuse/cartwheel_train/blob/master/CustomNets.py\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "import keras\n",
        "import code\n",
        "import numpy as np\n",
        "class NetVLADLayer(Layer):\n",
        "\n",
        "    def __init__( self, num_clusters, **kwargs ):\n",
        "        self.num_clusters = num_clusters\n",
        "        super(NetVLADLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build( self, input_shape ):\n",
        "        self.K = self.num_clusters\n",
        "        self.D = input_shape[-1]\n",
        "\n",
        "\n",
        "        self.kernel = self.add_weight( name='kernel',\n",
        "                                    shape=(1,1,self.D,self.K),\n",
        "                                    initializer='uniform',\n",
        "                                    trainable=True )\n",
        "\n",
        "        self.bias = self.add_weight( name='bias',\n",
        "                                    shape=(1,1,self.K),\n",
        "                                    initializer='uniform',\n",
        "                                    trainable=True )\n",
        "\n",
        "        self.C = self.add_weight( name='cluster_centers',\n",
        "                                shape=[1,1,1,self.D,self.K],\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "\n",
        "\n",
        "    def call( self, x ):\n",
        "        s = K.conv2d( x, self.kernel, padding='same' ) + self.bias\n",
        "        a = K.softmax( s )\n",
        "        self.amap = K.argmax( a, -1 )\n",
        "        a = K.expand_dims( a, -2 ) \n",
        "        v = K.expand_dims(x, -1) + self.C \n",
        "        v = a * v\n",
        "        v = K.sum(v, axis=[1, 2])\n",
        "        v = K.permute_dimensions(v, pattern=[0, 2, 1])\n",
        "\n",
        "        v = K.l2_normalize( v, axis=-1 )\n",
        "        v = K.batch_flatten( v )\n",
        "        v = K.l2_normalize( v, axis=-1 )\n",
        "        \n",
        "        # return [v, self.amap]\n",
        "        return v\n",
        "\n",
        "    def compute_output_shape( self, input_shape ):\n",
        "        return (input_shape[0], self.K*self.D )\n",
        "\n",
        "    def get_config( self ):\n",
        "        pass\n",
        "        # base_config = super(NetVLADLayer, self).get_config()\n",
        "        # return dict(list(base_config.items()))\n",
        "\n",
        "        # As suggested by: https://github.com/keras-team/keras/issues/4871#issuecomment-269731817\n",
        "        config = {'num_clusters': self.num_clusters}\n",
        "        base_config = super(NetVLADLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu8QUOtEhUi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/shamangary/LOUPE_Keras/blob/master/loupe_keras.py\n",
        "import math\n",
        "from keras import initializers, layers\n",
        "class NetVLAD(layers.Layer):\n",
        "    \"\"\"Creates a NetVLAD class.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_size, max_samples, cluster_size, output_dim, **kwargs):\n",
        "        \n",
        "        self.feature_size = feature_size\n",
        "        self.max_samples = max_samples\n",
        "        self.output_dim = output_dim\n",
        "        self.cluster_size = cluster_size\n",
        "        super(NetVLAD, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "    # Create a trainable weight variable for this layer.\n",
        "        self.cluster_weights = self.add_weight(name='kernel_W1',\n",
        "                                      shape=(self.feature_size, self.cluster_size),\n",
        "                                      initializer=tf.random_normal_initializer(stddev=1 / math.sqrt(self.feature_size)),\n",
        "                                      trainable=True)\n",
        "        self.cluster_biases = self.add_weight(name='kernel_B1',\n",
        "                                      shape=(self.cluster_size,),\n",
        "                                      initializer=tf.random_normal_initializer(stddev=1 / math.sqrt(self.feature_size)),\n",
        "                                      trainable=True)\n",
        "        self.cluster_weights2 = self.add_weight(name='kernel_W2',\n",
        "                                      shape=(1,self.feature_size, self.cluster_size),\n",
        "                                      initializer=tf.random_normal_initializer(stddev=1 / math.sqrt(self.feature_size)),\n",
        "                                      trainable=True)\n",
        "        self.hidden1_weights = self.add_weight(name='kernel_H1',\n",
        "                                      shape=(self.cluster_size*self.feature_size, self.output_dim),\n",
        "                                      initializer=tf.random_normal_initializer(stddev=1 / math.sqrt(self.cluster_size)),\n",
        "                                      trainable=True)\n",
        "        \n",
        "        super(NetVLAD, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, reshaped_input):\n",
        "        activation = K.dot(reshaped_input, self.cluster_weights)\n",
        "        \n",
        "        activation += self.cluster_biases\n",
        "        \n",
        "        activation = tf.nn.softmax(activation)\n",
        "\n",
        "        activation = tf.reshape(activation,\n",
        "                [-1, self.max_samples, self.cluster_size])\n",
        "\n",
        "        a_sum = tf.reduce_sum(activation,-2,keep_dims=True)\n",
        "        \n",
        "        a = tf.multiply(a_sum,self.cluster_weights2)\n",
        "        \n",
        "        activation = tf.transpose(activation,perm=[0,2,1])\n",
        "        \n",
        "        reshaped_input = tf.reshape(reshaped_input,[-1,\n",
        "            self.max_samples, self.feature_size])\n",
        "\n",
        "        vlad = tf.matmul(activation,reshaped_input)\n",
        "        vlad = tf.transpose(vlad,perm=[0,2,1])\n",
        "        vlad = tf.subtract(vlad,a)\n",
        "        vlad = tf.nn.l2_normalize(vlad,1)\n",
        "        vlad = tf.reshape(vlad,[-1, self.cluster_size*self.feature_size])\n",
        "        vlad = tf.nn.l2_normalize(vlad,1)\n",
        "        vlad = K.dot(vlad, self.hidden1_weights)\n",
        "\n",
        "        return vlad\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.output_dim])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO1xNEeSCn8W",
        "colab_type": "text"
      },
      "source": [
        "## base model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewgFWgXwiFlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size= 32\n",
        "input_shape = (128,128,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbH38BHTnKHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import Model, Input\n",
        "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten,MaxPooling2D, concatenate\n",
        "from keras.optimizers import SGD,Adam\n",
        "from keras.models import Sequential\n",
        "\n",
        "def initialize_model(backbone= None, input_shape=input_shape, use_pretrained = 'imagenet'):\n",
        "  base_model = None\n",
        "  dim = 0\n",
        "  base_model = backbone(input_shape=input_shape, weights=use_pretrained, include_top= False)\n",
        "  x = base_model.output\n",
        "  dim = x[-1].get_shape().as_list()[2]\n",
        "#   reshaped_input = tf.reshape(x, [-1, features_size])\n",
        "#   dim = reshaped_input[-1].get_shape().as_list()[0]\n",
        "  return base_model, dim\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp1QfMAemJZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## VGG16 or Xception or DenseNet121 or \n",
        "## DenseNet169 or DenseNet201 or NASNetMobile or \n",
        "## ResNet50 or InceptionResNetV2\n",
        "model_name = VGG16  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9fpB2YFmQe8",
        "colab_type": "code",
        "outputId": "89c17b7b-e418-4258-cf5b-cd7725dc28ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "base_model, dim =initialize_model(model_name, input_shape=input_shape)\n",
        "\n",
        "print('dim : ',dim)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "dim :  512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwqnzydo4cfH",
        "colab_type": "text"
      },
      "source": [
        "## Triplet Sampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsEOaJlgzOGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_triplet_num = 50\n",
        "test_triplet_num = 10\n",
        "img_width = 128\n",
        "img_height = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgGfXZwDzZPT",
        "colab_type": "text"
      },
      "source": [
        "### Train Triplets Pathes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy1ULMQOzdj9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "62deaa8d-d2bb-45bb-ba50-b0614ffee9ac"
      },
      "source": [
        "train_dir_pathes = []\n",
        "i =0\n",
        "for root,dir,file in os.walk(train_path):\n",
        "  if i>=1:\n",
        "    train_dir_pathes.append(root)\n",
        "  i+=1\n",
        "train_dir_pathes=sorted(train_dir_pathes)\n",
        "print(train_dir_pathes) # class dir path"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./gdrive/My Drive/train/AI', './gdrive/My Drive/train/Clock tower', './gdrive/My Drive/train/Front door Child', './gdrive/My Drive/train/Front door Sejong', './gdrive/My Drive/train/Museum', './gdrive/My Drive/train/Stone statue', './gdrive/My Drive/train/dorm']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ix4sckzzoN7",
        "colab_type": "text"
      },
      "source": [
        "### Test Triplets Pathes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8yX1tzDndPG",
        "colab_type": "code",
        "outputId": "67c76796-c167-42e8-8b1e-7393abf6c5d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "test_dir_pathes = []\n",
        "i =0\n",
        "for root,dir,file in os.walk(test_path):\n",
        "  if i>=1:\n",
        "    test_dir_pathes.append(root)\n",
        "  i+=1\n",
        "test_dir_pathes=sorted(test_dir_pathes)\n",
        "print(test_dir_pathes) # class dir path"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./gdrive/My Drive/test/AI', './gdrive/My Drive/test/Clock tower', './gdrive/My Drive/test/Front door Child', './gdrive/My Drive/test/Front door Sejong', './gdrive/My Drive/test/Museum', './gdrive/My Drive/test/Stone statue', './gdrive/My Drive/test/dorm']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4DvC7wS5g10",
        "colab_type": "code",
        "outputId": "5bb9e3cd-fd56-4264-d33f-e22199f00c76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "def get_triplets(triplet_num,img_width,img_height,dir_pathes):\n",
        "  if 'train' in str(dir_pathes) :\n",
        "    print('getting train data..!')\n",
        "  else :\n",
        "    print('getting test data..!')\n",
        "  total_len =len(dir_pathes)*triplet_num\n",
        "  print('total triplets length : {}'.format(total_len))\n",
        "  triplets = np.zeros((total_len,3,img_width,img_height,3))\n",
        "\n",
        "  triplets_len = 0\n",
        "  for i in range(len(dir_pathes)):\n",
        "      # set the files of anchor and positive images\n",
        "      anc_pos_files = [f for f in sorted(os.listdir(dir_pathes[i]))]\n",
        "      for iteration in range(triplet_num):\n",
        "\n",
        "          # set the files of negative images\n",
        "          j = random.choice([num for num in range(len(dir_pathes)) if num not in [i]])\n",
        "          nega_files = [o for o in sorted(os.listdir(dir_pathes[j]))]\n",
        "\n",
        "          # get anchor and positive images as numpy array\n",
        "          pair = np.random.randint(0,len(anc_pos_files),2)\n",
        "#           print('anchor :',dir_pathes[i],anc_pos_files[pair[0]])\n",
        "          anchor = np.array(Image.open('{0}/{1}'.format(dir_pathes[i],anc_pos_files[pair[0]])).resize((img_width,img_height)))\n",
        "#           print('positive :',dir_pathes[i],anc_pos_files[pair[1]])\n",
        "          positive = np.array(Image.open('{0}/{1}'.format(dir_pathes[i],anc_pos_files[pair[1]])).resize((img_width,img_height)))\n",
        "          \n",
        "          # get negative images as numpy array\n",
        "          \n",
        "          nega_idx = np.random.randint(len(nega_files))\n",
        "#           print('negative :',dir_pathes[j],nega_files[nega_idx])\n",
        "          negative = np.array(Image.open('{0}/{1}'.format(dir_pathes[j],nega_files[nega_idx])).resize((img_width,img_height)))\n",
        "          \n",
        "          # if image channel !=3 , resample triplets\n",
        "          if anchor.shape[2]!=3 or positive.shape[2]!=3 or negative.shape[2]!=3:\n",
        "            # get anchor and positive images as numpy array\n",
        "            pair = np.random.randint(0,len(anc_pos_files),2)\n",
        "            anchor = np.array(Image.open('{0}/{1}'.format(dir_pathes[i],anc_pos_files[pair[0]])).resize((img_width,img_height)))\n",
        "\n",
        "            positive = np.array(Image.open('{0}/{1}'.format(dir_pathes[i],anc_pos_files[pair[1]])).resize((img_width,img_height)))\n",
        "\n",
        "            # get negative images as numpy array\n",
        "            nega_idx = np.random.randint(len(nega_files))\n",
        "            negative = np.array(Image.open('{0}/{1}'.format(dir_pathes[j],nega_files[nega_idx])).resize((img_width,img_height)))\n",
        "          # (optional)visualization\n",
        "          # plot_triplet(triplet)\n",
        "\n",
        "          triplets[triplets_len:triplets_len+1,0:1,:,:,:]=anchor/255.\n",
        "          triplets[triplets_len:triplets_len+1,1:2,:,:,:]=positive/255.\n",
        "          triplets[triplets_len:triplets_len+1,2:3,:,:,:]=negative/255.\n",
        "\n",
        "          triplets_len+=1\n",
        "          if triplets_len%20==0:\n",
        "            print('{}/{}'.format(triplets_len,total_len))\n",
        "            \n",
        "  return triplets\n",
        "\n",
        "x_train = get_triplets(train_triplet_num,img_width,img_height,train_dir_pathes)\n",
        "x_test = get_triplets(test_triplet_num,img_width,img_height,test_dir_pathes)\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "getting train data..!\n",
            "total triplets length : 350\n",
            "20/350\n",
            "40/350\n",
            "60/350\n",
            "80/350\n",
            "100/350\n",
            "120/350\n",
            "140/350\n",
            "160/350\n",
            "180/350\n",
            "200/350\n",
            "220/350\n",
            "240/350\n",
            "260/350\n",
            "280/350\n",
            "300/350\n",
            "320/350\n",
            "340/350\n",
            "getting test data..!\n",
            "total triplets length : 70\n",
            "20/70\n",
            "40/70\n",
            "60/70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUcso0DjlaZR",
        "colab_type": "code",
        "outputId": "d6245269-4af0-4e73-f537-f969ff6ee33b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(type(x_train))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uOEUDvumvO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "triplet_dir = './gdrive/My Drive/triplets'\n",
        "import os\n",
        "if os.path.exists(triplet_dir)==False:\n",
        "  os.mkdir(triplet_dir)\n",
        "else:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16u8HP3KlWqb",
        "colab_type": "code",
        "outputId": "aeea3cf6-728c-486b-ee56-521126a3435c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# save triplets as numpy array\n",
        "train_triplet_path = os.path.join(triplet_dir,'x_train.npy')\n",
        "test_triplet_path = os.path.join(triplet_dir,'x_test.npy')\n",
        "print('save file')\n",
        "np.save(train_triplet_path,x_train)\n",
        "np.save(test_triplet_path,x_test)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "save file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvejfqGVyT6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x_train = np.load(train_triplets_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXR5WO1WFmJh",
        "colab_type": "text"
      },
      "source": [
        "## Loss and Entire Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmQuSWzFFpJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##https://codepad.co/snippet/triplet-loss-in-keras-tensorflow-backend\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, concatenate,merge\n",
        "from keras.optimizers import Adam\n",
        "from keras import optimizers\n",
        "ALPHA = 0.2  # Triplet Loss Parameter\n",
        "\n",
        "def triplet_loss(x):\n",
        "    anchor, positive, negative = x\n",
        "\n",
        "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n",
        "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n",
        "\n",
        "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), ALPHA)\n",
        "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n",
        "\n",
        "    return loss\n",
        "  \n",
        "def lossless_triplet_loss(y_true, y_pred, N = 4608, epsilon=1e-8):\n",
        "    \"\"\"\n",
        "    Implementation of the triplet loss function\n",
        "    \n",
        "    Arguments:\n",
        "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
        "    y_pred -- python list containing three objects:\n",
        "            anchor -- the encodings for the anchor data\n",
        "            positive -- the encodings for the positive data (similar to anchor)\n",
        "            negative -- the encodings for the negative data (different from anchor)\n",
        "    N  --  The number of dimension \n",
        "    beta -- The scaling factor, N is recommended\n",
        "    epsilon -- The Epsilon value to prevent ln(0)\n",
        "    \n",
        "    \n",
        "    Returns:\n",
        "    loss -- real number, value of the loss\n",
        "    \"\"\"\n",
        "    anchor = y_pred[:,0:N]\n",
        "    print(anchor.shape)\n",
        "    positive = y_pred[:,N:N*2]\n",
        "    print(positive.shape)\n",
        "    negative = y_pred[:,N*2:N*3]\n",
        "    print(negative.shape)\n",
        "    # distance between the anchor and the positive\n",
        "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),1)\n",
        "    \n",
        "    # distance between the anchor and the negative\n",
        "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),1)\n",
        "    \n",
        "    #Non Linear Values  \n",
        "    \n",
        "    # -ln(-x/N+1)\n",
        "    pos_dist = -tf.log(-tf.divide((pos_dist),N)+1+epsilon)\n",
        "    neg_dist = -tf.log(-tf.divide((N-neg_dist),N)+1+epsilon)\n",
        "    \n",
        "    # compute loss\n",
        "    loss = neg_dist + pos_dist\n",
        "    triplet_loss = tf.reduce_mean(tf.maximum(pos_dist - neg_dist + epsilon, 0.))\n",
        "    print(loss)\n",
        "    return triplet_loss\n",
        "  \n",
        "def load_model(input_shape,base_model,net_vlad):\n",
        "    \n",
        "    anchor = Input(shape=(input_shape[0],input_shape[1],input_shape[2]),name='anchor_input')\n",
        "    pos = Input(shape=(input_shape[0],input_shape[1],input_shape[2]),name='positive_input')\n",
        "    neg = Input(shape=(input_shape[0],input_shape[1],input_shape[2]),name='negative_input')\n",
        "    \n",
        "    anchor_feature=base_model(anchor)\n",
        "    pos_feature=base_model(pos)\n",
        "    neg_feature=base_model(neg)\n",
        "    \n",
        "    anchor_vlad = net_vlad(anchor_feature)\n",
        "    pos_vlad = net_vlad(pos_feature)\n",
        "    neg_vlad = net_vlad(neg_feature)\n",
        "    \n",
        "    \n",
        "    \n",
        "#     loss = merge([anchor_vlad, pos_vlad, neg_vlad],mode=triplet_loss, name='loss', output_shape=(1,))\n",
        "    \n",
        "#     model = Model(inputs=[anchor_example, positive_example, negative_example],\n",
        "#                   outputs=loss)\n",
        "    merged_vector = concatenate([anchor_vlad, pos_vlad, neg_vlad], axis=-1)\n",
        "\n",
        "    # Define the trainable model\n",
        "    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    \n",
        "    model = Model(inputs=[anchor, pos, neg], outputs=merged_vector)\n",
        "#     model.compile(optimizer=Adam(), loss=triplet_loss, metrics=[accuracy])\n",
        "    model.compile(optimizer=sgd,loss=lossless_triplet_loss)\n",
        "#     model.compile(loss='mean_absolute_error', optimizer=Adam())\n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZksGSuLg9Xd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xa=base_model(tf.convert_to_tensor(x_train[0:5,0], np.float32))\n",
        "xp=base_model(tf.convert_to_tensor(x_train[0:5,1], np.float32))\n",
        "xn=base_model(tf.convert_to_tensor(x_train[0:5,2], np.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YekpOIUbrndm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape=(128,128,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5hdgOkQXB9C",
        "colab_type": "code",
        "outputId": "18706767-5c40-4de3-b4dc-89209f14dca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "net_vlad = NetVLADLayer(16)\n",
        "model = load_model(input_shape,base_model,net_vlad)\n"
      ],
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, ?)\n",
            "(?, ?)\n",
            "(?, ?)\n",
            "Tensor(\"loss_33/concatenate_47_loss/add_4:0\", shape=(?,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpCGXBVDuV-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=net_vlad(xa)\n",
        "p=net_vlad(xp)\n",
        "n=net_vlad(xn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYedmG8wucE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xe=concatenate([a,p,n],axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8svIYbCqLki1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess=tf.Session()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcMlzDlhLXNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "array = xe.eval(session=sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtPRAoYZLuTU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "cf4ce22a-c79b-4a50-ed02-aa51e2110fdc"
      },
      "source": [
        "array.shape\n",
        "print(array[0:4608])"
      ],
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.00420239  0.00097386 -0.00864621 ...  0.01454314 -0.01106594\n",
            "  -0.01862521]\n",
            " [ 0.00432085  0.00106746 -0.00864508 ...  0.01444227 -0.01109951\n",
            "  -0.01861683]\n",
            " [ 0.0041827   0.00098127 -0.00864642 ...  0.01432618 -0.01106929\n",
            "  -0.01862514]\n",
            " [ 0.00424047  0.00102774 -0.00862667 ...  0.01468687 -0.01107235\n",
            "  -0.0186138 ]\n",
            " [ 0.00430996  0.00100277 -0.00866738 ...  0.01453572 -0.0110676\n",
            "  -0.01862215]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsWefpj1MiPQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3952c1d8-78b1-469f-eb89-8520ec1be516"
      },
      "source": [
        "print(array[4608:9216])"
      ],
      "execution_count": 374,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pglm-qJlC35V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "31e517a0-423e-4416-ad0f-8481caa5fe3e"
      },
      "source": [
        "pred=lossless_triplet_loss(0,xe)"
      ],
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4608)\n",
            "(5, 4608)\n",
            "(5, 4608)\n",
            "Tensor(\"add_47:0\", shape=(5,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhTI68VYCW6j",
        "colab_type": "code",
        "outputId": "690664bc-ed42-427f-84b6-cf4dd2e74a66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "anchor_input (InputLayer)       (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "positive_input (InputLayer)     (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "negative_input (InputLayer)     (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "vgg16 (Model)                   multiple             14714688    anchor_input[0][0]               \n",
            "                                                                 positive_input[0][0]             \n",
            "                                                                 negative_input[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "net_vlad_layer_34 (NetVLADLayer multiple             9225        vgg16[130][0]                    \n",
            "                                                                 vgg16[131][0]                    \n",
            "                                                                 vgg16[132][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 13824)        0           net_vlad_layer_34[0][0]          \n",
            "                                                                 net_vlad_layer_34[1][0]          \n",
            "                                                                 net_vlad_layer_34[2][0]          \n",
            "==================================================================================================\n",
            "Total params: 14,723,913\n",
            "Trainable params: 14,723,913\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdX2vrygVlzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtfTY8eeXt0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def full_auc(model, ground_truth):\n",
        "    \"\"\"\n",
        "    Measure AUC for model and ground truth on all items.\n",
        "    Returns:\n",
        "    - float AUC\n",
        "    \"\"\"\n",
        "\n",
        "    ground_truth = ground_truth.tocsr()\n",
        "\n",
        "    no_users, no_items = ground_truth.shape\n",
        "\n",
        "    pid_array = np.arange(no_items, dtype=np.int32)\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for user_id, row in enumerate(ground_truth):\n",
        "\n",
        "        predictions = predict(model, user_id, pid_array)\n",
        "\n",
        "        true_pids = row.indices[row.data == 1]\n",
        "\n",
        "        grnd = np.zeros(no_items, dtype=np.int32)\n",
        "        grnd[true_pids] = 1\n",
        "\n",
        "        if len(true_pids):\n",
        "            scores.append(roc_auc_score(grnd, predictions))\n",
        "\n",
        "    return sum(scores) / len(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf8j5hR49nmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_len=x_train.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey2zqITbTxT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "892dfa4d-616c-408b-8dd8-daab9f5c1022"
      },
      "source": [
        "# steps_per_epoch * batch size = dataset size\n",
        "steps_per_epoch = int(total_len/ batch_size)\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    print('Epoch %s' % epoch)\n",
        "\n",
        "    # Sample triplets from the training data\n",
        "    a=tf.convert_to_tensor(x_train[epoch*batch_size:(epoch+1)*batch_size,0], np.float32)\n",
        "    p = tf.convert_to_tensor(x_train[epoch*batch_size:(epoch+1)*batch_size,1], np.float32)\n",
        "    n = tf.convert_to_tensor(x_train[epoch*batch_size:(epoch+1)*batch_size,2], np.float32)\n",
        "\n",
        "    X = {\n",
        "        'anchor_input': a,\n",
        "        'positive_input': p,\n",
        "        'negative_input': n\n",
        "    }\n",
        "\n",
        "    hist= model.fit(X,\n",
        "              np.ones(a.shape),\n",
        "              verbose=0,\n",
        "              steps_per_epoch=steps_per_epoch,\n",
        "              shuffle=True)\n",
        "    print(hist.history['loss'])\n",
        "\n",
        "#     print('AUC %s' % full_auc(model, x_test[epoch]))"
      ],
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "[0.0]\n",
            "Epoch 1\n",
            "[0.0]\n",
            "Epoch 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-331-66d86fef40a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m               shuffle=True)\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1468\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \"\"\"\n\u001b[0;32m-> 1470\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1471\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuNGY4QUyDU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}