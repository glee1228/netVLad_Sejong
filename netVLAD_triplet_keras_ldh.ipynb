{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "netVLAD_triplet_keras_ldh.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glee1228/net_VLad_keras/blob/master/netVLAD_triplet_keras_ldh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcX-54sSCQf9",
        "colab_type": "text"
      },
      "source": [
        "## 코랩 연동"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOMyGNH7ULRF",
        "colab_type": "code",
        "outputId": "7e9c0e72-6f67-489c-b53e-96c2f20359b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.applications.xception import Xception\n",
        "from keras.applications.densenet import DenseNet121, DenseNet169, DenseNet201\n",
        "from keras.applications.nasnet import NASNetMobile\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.applications import VGG16\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD8vfj9mX33K",
        "colab_type": "code",
        "outputId": "bbce9a9e-cd24-4a4e-d7c9-040c2008e748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install tensorboardcolab\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "tb = TensorBoardColab()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n",
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://a0b3bc0f.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu1jT3RYwxeQ",
        "colab_type": "code",
        "outputId": "07018b84-f9e9-4121-cf5d-e1f65933398a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if os.path.exists('/content/gdrive')==False:\n",
        "    drive.mount('/content/gdrive')\n",
        "    print('Google Drive is mounted\\n')\n",
        "else:\n",
        "    print('Google Drive is already mounted\\n')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google Drive is already mounted\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhko5ZnxYM3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.set_random_seed(777)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVbt03PPCUgG",
        "colab_type": "text"
      },
      "source": [
        "## 데이터 전처리 후 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7qv_b9jUqAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prof_team = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVXFtGxtVGPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if prof_team :\n",
        "  ckpt_path = '/content/gdrive/My Drive/AILeader_Dataset/Checkpoint_jw'\n",
        "  train_path = './gdrive/My Drive/AILeader_Dataset/train'\n",
        "  test_path = './gdrive/My Drive/AILeader_Dataset/test'\n",
        "  \n",
        "else :\n",
        "  ckpt_path = './gdrive/My Drive/ckpt'\n",
        "  train_path = './gdrive/My Drive/train'\n",
        "  test_path = './gdrive/My Drive/test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohZAm7Ugw01q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES']= '0'\n",
        "data_dir = './gdrive/My Drive/AILeader_Dataset'\n",
        "resized_data_dir = './gdrive/My Drive/resized_Dataset'\n",
        "resized_data_path = ['./gdrive/My Drive/resized_Dataset/train_x.npy','./gdrive/My Drive/resized_Dataset/train_y.npy']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi-bsA0x8qCp",
        "colab_type": "code",
        "outputId": "7aef5bb7-caf5-4f28-a1f5-faba59952230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "! ls -a ./gdrive/My\\ Drive/train"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " AI\t        dorm\t\t   'Front door Sejong'\t'Stone statue'\n",
            "'Clock tower'  'Front door Child'   Museum\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn1lec3N5bcC",
        "colab_type": "text"
      },
      "source": [
        "## NetVLAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tcd0GVzj5d7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "import keras\n",
        "import code\n",
        "import numpy as np\n",
        "class NetVLADLayer(Layer):\n",
        "\n",
        "    def __init__( self, num_clusters, **kwargs ):\n",
        "        self.num_clusters = num_clusters\n",
        "        super(NetVLADLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build( self, input_shape ):\n",
        "        self.K = self.num_clusters\n",
        "        self.D = input_shape[-1]\n",
        "\n",
        "\n",
        "        self.kernel = self.add_weight( name='kernel',\n",
        "                                    shape=(1,1,self.D,self.K),\n",
        "                                    initializer='uniform',\n",
        "                                    trainable=True )\n",
        "\n",
        "        self.bias = self.add_weight( name='bias',\n",
        "                                    shape=(1,1,self.K),\n",
        "                                    initializer='uniform',\n",
        "                                    trainable=True )\n",
        "\n",
        "        self.C = self.add_weight( name='cluster_centers',\n",
        "                                shape=[1,1,1,self.D,self.K],\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "\n",
        "\n",
        "    def call( self, x ):\n",
        "        s = K.conv2d( x, self.kernel, padding='same' ) + self.bias\n",
        "        a = K.softmax( s )\n",
        "        self.amap = K.argmax( a, -1 )\n",
        " \n",
        "        a = K.expand_dims( a, -2 ) \n",
        "        v = K.expand_dims(x, -1) + self.C \n",
        "        v = a * v\n",
        "        v = K.sum(v, axis=[1, 2])\n",
        "        v = K.permute_dimensions(v, pattern=[0, 2, 1])\n",
        "\n",
        "        v = K.l2_normalize( v, axis=-1 )\n",
        "        v = K.batch_flatten( v )\n",
        "        v = K.l2_normalize( v, axis=-1 )\n",
        "\n",
        "        # return [v, self.amap]\n",
        "        return v\n",
        "\n",
        "    def compute_output_shape( self, input_shape ):\n",
        "        return (input_shape[0], self.K*self.D )\n",
        "\n",
        "    def get_config( self ):\n",
        "        pass\n",
        "        # base_config = super(NetVLADLayer, self).get_config()\n",
        "        # return dict(list(base_config.items()))\n",
        "\n",
        "        # As suggested by: https://github.com/keras-team/keras/issues/4871#issuecomment-269731817\n",
        "        config = {'num_clusters': self.num_clusters}\n",
        "        base_config = super(NetVLADLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsrWj4VDQ0WG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def triplet_loss(y_true, y_pred, alpha = 0.4):\n",
        "    \n",
        "    print('y_pred.shape = ',y_pred)\n",
        "    \n",
        "    total_lenght = y_pred.shape.as_list()[-1]\n",
        "#     print('total_lenght=',  total_lenght)\n",
        "#     total_lenght =12\n",
        "    \n",
        "    anchor = y_pred[:,0:int(total_lenght*1/3)]\n",
        "    positive = y_pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "    negative = y_pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "\n",
        "    # distance between the anchor and the positive\n",
        "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
        "\n",
        "    # distance between the anchor and the negative\n",
        "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
        "\n",
        "    # compute loss\n",
        "    basic_loss = pos_dist-neg_dist+alpha\n",
        "    loss = K.maximum(basic_loss,0.0)\n",
        " \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO1xNEeSCn8W",
        "colab_type": "text"
      },
      "source": [
        "## base model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewgFWgXwiFlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size= 32\n",
        "input_shape = (128,128,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbH38BHTnKHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import Model, Input\n",
        "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten,MaxPooling2D, concatenate\n",
        "from keras.optimizers import SGD,Adam\n",
        "from keras.models import Sequential\n",
        "\n",
        "def initialize_model(backbone= None, input_shape=input_shape, use_pretrained = 'imagenet'):\n",
        "  base_model = None\n",
        "  dim = 0\n",
        "  base_model = backbone(input_shape=input_shape, weights=use_pretrained, include_top= False)\n",
        "  x = base_model.output\n",
        "  dim = x[-1].get_shape().as_list()[2]\n",
        "  \n",
        "  return base_model, dim\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp1QfMAemJZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## VGG16 or Xception or DenseNet121 or \n",
        "## DenseNet169 or DenseNet201 or NASNetMobile or \n",
        "## ResNet50 or InceptionResNetV2\n",
        "model_name = VGG16  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9fpB2YFmQe8",
        "colab_type": "code",
        "outputId": "827fea8c-1184-4de5-89aa-b431ea9fe7a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "base_model, dim =initialize_model(model_name, input_shape=input_shape)\n",
        "\n",
        "print('dim : ',dim)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "dim :  512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwqnzydo4cfH",
        "colab_type": "text"
      },
      "source": [
        "## Triplet Sampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8yX1tzDndPG",
        "colab_type": "code",
        "outputId": "d1b25c28-54fa-4f5e-883a-25d919c76be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dir_pathes = []\n",
        "i =0\n",
        "for root,dir,file in os.walk(train_path):\n",
        "  if i>=1:\n",
        "    dir_pathes.append(root)\n",
        "  i+=1\n",
        "print(dir_pathes) # class dir path"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./gdrive/My Drive/train/Front door Child', './gdrive/My Drive/train/Museum', './gdrive/My Drive/train/AI', './gdrive/My Drive/train/Front door Sejong', './gdrive/My Drive/train/Stone statue', './gdrive/My Drive/train/Clock tower', './gdrive/My Drive/train/dorm']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4DvC7wS5g10",
        "colab_type": "code",
        "outputId": "76c1b58c-dbed-4698-f691-8dd419bc3bd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "triplet_num = 50\n",
        "img_width = 128\n",
        "img_height = 128\n",
        "total_len =len(dir_pathes)*triplet_num\n",
        "print('total triplets length : {}'.format(total_len))\n",
        "triplets = np.zeros((total_len,3,img_width,img_height,3))\n",
        "\n",
        "triplets_len = 0\n",
        "for i in range(len(dir_pathes)):\n",
        "    # set the files of anchor and positive images\n",
        "    anc_pos_files = [f for f in sorted(os.listdir(dir_pathes[i]))]\n",
        "    for iteration in range(triplet_num):\n",
        "\n",
        "        # set the files of negative images\n",
        "        j = random.choice([num for num in range(len(dir_pathes)) if num not in [i]])\n",
        "        nega_files = [o for o in sorted(os.listdir(dir_pathes[j]))]\n",
        "        \n",
        "        # get anchor and positive images as numpy array\n",
        "        pair = np.random.randint(0,len(anc_pos_files),2)\n",
        "        anchor = np.array(Image.open('{0}/{1}'.format(dir_pathes[i],anc_pos_files[pair[0]])).resize((img_width,img_height)))\n",
        "          \n",
        "        positive = np.array(Image.open('{0}/{1}'.format(dir_pathes[i],anc_pos_files[pair[1]])).resize((img_width,img_height)))\n",
        "\n",
        "        # get negative images as numpy array\n",
        "        nega_idx = np.random.randint(len(nega_files))\n",
        "        negative = np.array(Image.open('{0}/{1}'.format(dir_pathes[j],nega_files[nega_idx])).resize((img_width,img_height)))\n",
        "        \n",
        "        # if image channel !=3 , resample triplets\n",
        "        if anchor.shape[2]!=3 or positive.shape[2]!=3 or negative.shape[2]!=3:\n",
        "          # get anchor and positive images as numpy array\n",
        "          pair = np.random.randint(0,len(anc_pos_files),2)\n",
        "          anchor = np.array(Image.open('{0}/{1}'.format(dir_pathes[i],anc_pos_files[pair[0]])).resize((img_width,img_height)))\n",
        "          \n",
        "          positive = np.array(Image.open('{0}/{1}'.format(dir_pathes[i],anc_pos_files[pair[1]])).resize((img_width,img_height)))\n",
        "\n",
        "          # get negative images as numpy array\n",
        "          nega_idx = np.random.randint(len(nega_files))\n",
        "          negative = np.array(Image.open('{0}/{1}'.format(dir_pathes[j],nega_files[nega_idx])).resize((img_width,img_height)))\n",
        "        # (optional)visualization\n",
        "        # plot_triplet(triplet)\n",
        "        \n",
        "        triplets[triplets_len:triplets_len+1,0:1,:,:,:]=anchor/255.\n",
        "        triplets[triplets_len:triplets_len+1,1:2,:,:,:]=positive/255.\n",
        "        triplets[triplets_len:triplets_len+1,2:3,:,:,:]=negative/255.\n",
        "        \n",
        "        triplets_len+=1\n",
        "        if triplets_len%20==0:\n",
        "          print('{}/{}'.format(triplets_len,total_len))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total triplets length : 350\n",
            "20/350\n",
            "40/350\n",
            "60/350\n",
            "80/350\n",
            "100/350\n",
            "120/350\n",
            "140/350\n",
            "160/350\n",
            "180/350\n",
            "200/350\n",
            "220/350\n",
            "240/350\n",
            "260/350\n",
            "280/350\n",
            "300/350\n",
            "320/350\n",
            "340/350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUcso0DjlaZR",
        "colab_type": "code",
        "outputId": "8543586a-9502-46e1-c3ce-32b9b6451251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "triplets.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(350, 3, 128, 128, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uOEUDvumvO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "triplets_dir = './gdrive/My Drive/triplets'\n",
        "import os\n",
        "if os.path.exists(triplets_dir)==False:\n",
        "  os.mkdir(triplets_dir)\n",
        "else:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16u8HP3KlWqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a2b2cde-9807-48b1-c10a-dc4ba62cf4b7"
      },
      "source": [
        "# save triplets as numpy array\n",
        "triplets_path = os.path.join(triplets_dir,'triplets.npy')\n",
        "print('save file')\n",
        "np.save(triplets_path,triplets)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "save file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvejfqGVyT6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# triplets = np.load(triplets_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG--VQpG1sRE",
        "colab_type": "text"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKF8IiXA2yxR",
        "colab_type": "code",
        "outputId": "65ae853a-9202-43c4-8a7a-84f264378040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "# def extract_features(directory, sample_count):\n",
        "#     features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
        "#     labels = np.zeros(shape=(sample_count))\n",
        "#     generator = datagen.flow_from_directory(\n",
        "#         directory,\n",
        "#         target_size=(128, 128),\n",
        "#         batch_size=batch_size,\n",
        "#         class_mode='binary')\n",
        "#     i = 0\n",
        "#     for inputs_batch, labels_batch in generator:\n",
        "#         features_batch = base_model.predict(inputs_batch)\n",
        "        \n",
        "#         features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "#         labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "#         i += 1\n",
        "#         if i * batch_size >= sample_count:\n",
        "#             break\n",
        "#     return features, labels\n",
        "\n",
        "# train_features, train_labels = extract_features(train_path, 500)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 749 images belonging to 7 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-26b8a759893b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                        class_mode='binary')\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriplets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# configure batch size and retrieve one batch of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriplets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, augment, rounds, seed)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             raise ValueError('Input to `.fit()` should have rank 4. '\n\u001b[0;32m--> 912\u001b[0;31m                              'Got array with shape: ' + str(x.shape))\n\u001b[0m\u001b[1;32m    913\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel_axis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValueError\u001b[0m: Input to `.fit()` should have rank 4. Got array with shape: (350, 3, 128, 128, 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjIi1ApF6GaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# if os.path.exists(resized_data_dir)==False:\n",
        "#   os.mkdir(resized_data_dir)\n",
        "# else:\n",
        "#   pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQunjiH-3Ah4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "2c279470-fcc3-4691-a0bf-25cfe90f0fe8"
      },
      "source": [
        "# np.save(resized_data_path[0],train_features)\n",
        "# np.save(resized_data_path[1],train_labels)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7c0f21fa6780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_data_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_data_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsJbSFjVlI_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_features = np.load(resized_data_path[0])\n",
        "# train_labels = np.load(resized_data_path[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AbAh3He3qNX",
        "colab_type": "code",
        "outputId": "535f9e53-a95c-4512-9370-1bf9441e0a13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# print(train_features.shape)\n",
        "# print(train_labels.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 4, 4, 512)\n",
            "(500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXR5WO1WFmJh",
        "colab_type": "text"
      },
      "source": [
        "## Loss and Entire Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmQuSWzFFpJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##https://codepad.co/snippet/triplet-loss-in-keras-tensorflow-backend\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, concatenate,merge\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "ALPHA = 0.2  # Triplet Loss Parameter\n",
        "\n",
        "def triplet_loss(x):\n",
        "    anchor, positive, negative = x\n",
        "\n",
        "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n",
        "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n",
        "\n",
        "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), ALPHA)\n",
        "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def load_model(input_shape,base_model,net_vlad):\n",
        "    anchor = Input(shape=input_shape)\n",
        "    pos = Input(shape=input_shape)\n",
        "    neg = Input(shape=input_shape)\n",
        "    \n",
        "    anchor_feature=base_model(anchor)\n",
        "    pos_feature=base_model(pos)\n",
        "    neg_feature=base_model(neg)\n",
        "    \n",
        "    anchor_vlad = net_vlad(anchor_feature)\n",
        "    pos_vlad = net_vlad(pos_feature)\n",
        "    neg_vlad = net_vlad(neg_feature)\n",
        "    \n",
        "    loss = merge([anchor_vlad, pos_vlad, neg_vlad],mode=triplet_loss, output_shape=(1,))\n",
        "    \n",
        "    model = Model(inputs=[anchor_example, positive_example, negative_example],\n",
        "                  outputs=loss)\n",
        "    \n",
        "    model.compile(loss='mean_absolute_error', optimizer=Adam())\n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4J1SMN1memT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras import metrics\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# datagen = ImageDataGenerator(rescale=1./255)\n",
        "# batch_size = 20\n",
        "# train_generator = datagen.flow_from_directory(train_path,\n",
        "#                                        target_size=(128,128),\n",
        "#                                        batch_size=batch_size,\n",
        "#                                        class_mode='binary')\n",
        "\n",
        "# inputs = None\n",
        "# labels = None\n",
        "# i = 0\n",
        "# for input_batches, input_labels in train_generator:\n",
        "#   if i==1:\n",
        "#     break\n",
        "#   else:\n",
        "#     inputs=input_batches\n",
        "#     labels=input_labels\n",
        "#   i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5hdgOkQXB9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "1493bc33-1a65-4f63-a4da-46e29bd3b930"
      },
      "source": [
        "net_vlad = NetVLADLayer(num_clusters=9)\n",
        "model = load_model((128,128,3),base_model,net_vlad)\n",
        "output=model(tf.convert_to_tensor(inputs, np.float32))\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-54975ca58409>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet_vlad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetVLADLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet_vlad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-389878cb1204>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(input_shape, base_model, net_vlad)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mneg_vlad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_vlad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manchor_vlad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_vlad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_vlad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtriplet_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     model = Model(inputs=[anchor_example, positive_example, negative_example],\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhTI68VYCW6j",
        "colab_type": "code",
        "outputId": "aa5dea50-8d59-407a-daaf-e5d494f7c866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "net_vlad_layer_8 (NetVLADLay (None, 4608)              9225      \n",
            "=================================================================\n",
            "Total params: 14,723,913\n",
            "Trainable params: 14,723,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}